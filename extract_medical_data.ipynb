{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-chroma langchain_google_genai langchain_community faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "am_CgzlN1wM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip show langchain langchain-chroma langchain_google_genai langchain_community faiss-cpu BeautifulSoup4 pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mfN_1ieLfUcP",
        "outputId": "b18eb442-fe8b-4501-903b-6a8066c3ee6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.13\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community\n",
            "---\n",
            "Name: langchain-chroma\n",
            "Version: 0.1.4\n",
            "Summary: An integration package connecting Chroma and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: chromadb, fastapi, langchain-core, numpy\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-google-genai\n",
            "Version: 2.0.7\n",
            "Summary: An integration package connecting Google's genai package and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain-google\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filetype, google-generativeai, langchain-core, pydantic\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.3.13\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: faiss-cpu\n",
            "Version: 1.9.0.post1\n",
            "Summary: A library for efficient similarity search and clustering of dense vectors.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Kota Yamaguchi <yamaguchi_kota@cyberagent.co.jp>\n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, packaging\n",
            "Required-by: \n",
            "---\n",
            "Name: beautifulsoup4\n",
            "Version: 4.12.3\n",
            "Summary: Screen-scraping library\n",
            "Home-page: https://www.crummy.com/software/BeautifulSoup/bs4/\n",
            "Author: \n",
            "Author-email: Leonard Richardson <leonardr@segfault.org>\n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: soupsieve\n",
            "Required-by: gdown, google, nbconvert, yfinance\n",
            "---\n",
            "Name: pydantic\n",
            "Version: 2.10.3\n",
            "Summary: Data validation using Python type hints\n",
            "Home-page: https://github.com/pydantic/pydantic\n",
            "Author: \n",
            "Author-email: Samuel Colvin <s@muelcolvin.com>, Eric Jolibois <em.jolibois@gmail.com>, Hasan Ramezani <hasan.r67@gmail.com>, Adrian Garcia Badaracco <1755071+adriangb@users.noreply.github.com>, Terrence Dorsey <terry@pydantic.dev>, David Montague <david@pydantic.dev>, Serge Matveenko <lig@countzero.co>, Marcelo Trylesinski <marcelotryle@gmail.com>, Sydney Runkle <sydneymarierunkle@gmail.com>, David Hewitt <mail@davidhewitt.io>, Alex Hall <alex.mojaki@gmail.com>, Victorien Plot <contact@vctrn.dev>\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: annotated-types, pydantic-core, typing-extensions\n",
            "Required-by: albumentations, chromadb, confection, fastapi, google-cloud-aiplatform, google-genai, google-generativeai, langchain, langchain-core, langchain-google-genai, langsmith, openai, pydantic-settings, spacy, thinc, wandb, weasel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ],
      "metadata": {
        "id": "nHtGoED91lbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import csv\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from pydantic import BaseModel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "\n",
        "# Initialize Google Generative AI Models\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-exp\")\n",
        "\n",
        "# CSV File paths\n",
        "VISITED_LINKS_FILE = \"visited_links.csv\"\n",
        "OUTPUT_CSV_FILE = \"medical_advices.csv\"\n",
        "\n",
        "# Ensure CSV files exist\n",
        "for file_path in [VISITED_LINKS_FILE, OUTPUT_CSV_FILE]:\n",
        "    if not os.path.exists(file_path):\n",
        "        with open(file_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            if file_path == VISITED_LINKS_FILE:\n",
        "                writer.writerow([\"link\"])\n",
        "            else:\n",
        "                writer.writerow([\"q_type\", \"question\", \"answer\"])\n",
        "\n",
        "def get_category_links(main_url):\n",
        "    \"\"\"\n",
        "    Scrape the main page to extract all category links.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(main_url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract category links\n",
        "        category_links = []\n",
        "        categories = soup.find_all('li', class_='link-list-items__deprecated')\n",
        "        for category in categories:\n",
        "            link_tag = category.find('a', class_='link-list-link__deprecated')\n",
        "            if link_tag:\n",
        "                link = link_tag.get('href')\n",
        "                if link and link.startswith('http'):\n",
        "                    category_links.append(link)\n",
        "\n",
        "        return category_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching category links: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_card_links(category_url):\n",
        "    \"\"\"\n",
        "    Scrape a category page to extract all card links.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(category_url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract card links from the category page\n",
        "        card_links = []\n",
        "        cards = soup.find_all('a', class_='mntl-card-list-items')\n",
        "        for card in cards:\n",
        "            link = card.get('href')\n",
        "            if link and link.startswith('http'):\n",
        "                card_links.append(link)\n",
        "\n",
        "        return card_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching card links from {category_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def scrape_card_content(card_url):\n",
        "    \"\"\"\n",
        "    Visit each card link and scrape the content from the specified <p> tag onward.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(card_url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Locate the starting <p> tag\n",
        "        start_paragraph = soup.find('p', id='mntl-sc-block_1-0')\n",
        "        if not start_paragraph:\n",
        "            print(f\"Start paragraph not found in {card_url}\")\n",
        "            return None\n",
        "\n",
        "        # Collect content from the starting point\n",
        "        content = \"\"\n",
        "        stop_phrases = [\"Summary\", \"A Word From Verywell\"]\n",
        "\n",
        "        # Traverse siblings from the starting point\n",
        "        for element in start_paragraph.find_all_next():\n",
        "            if element.name == \"h2\" and any(phrase in element.get_text() for phrase in stop_phrases):\n",
        "                break\n",
        "            content += element.get_text() + \"\\n\"\n",
        "\n",
        "        return content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {card_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_medical_advices_from_text(text):\n",
        "    \"\"\"\n",
        "    Generate meaningful data in the format q_type, question, answer.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    documents = text_splitter.split_documents([Document(page_content=text)])\n",
        "\n",
        "    db = FAISS.from_documents(documents, embeddings)\n",
        "    retriever = db.as_retriever()\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        Generate meaningful data from the extracted data in the json format:\n",
        "        [\n",
        "            {{\n",
        "                \"q_type\": \"treatment\",\n",
        "                \"question\": \"What are some discharge instructions following an asthma attack?\",\n",
        "                \"answer\": \"Avoid allergy triggers and follow up with their provider to adjust the asthma plan as needed.\"\n",
        "            }},\n",
        "            {{\n",
        "                \"q_type\": \"trigger\",\n",
        "                \"question\": \"What are some less common triggers for asthma attacks?\",\n",
        "                \"answer\": \"Cold and flu, sinus issues, strenuous exercise, weather changes, stress and emotional distress.\"\n",
        "            }}\n",
        "            etc..\n",
        "        ]\n",
        "\n",
        "        Remove * from the output.\n",
        "\n",
        "        Generate the data related only to the medical based only on this context: \\n\\n{context}\\n\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    response = rag_chain.invoke(\"\")\n",
        "    return response\n",
        "\n",
        "def append_to_csv(file_path, data, mode=\"a\"):\n",
        "    \"\"\"\n",
        "    Append data to a CSV file.\n",
        "    \"\"\"\n",
        "    with open(file_path, mode, newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "    main_url = \"https://www.verywellhealth.com/health-a-z-4014770\"\n",
        "    print(\"Fetching category links from the main page...\")\n",
        "\n",
        "    category_links = get_category_links(main_url)\n",
        "    if not category_links:\n",
        "        print(\"No category links found.\")\n",
        "        return\n",
        "\n",
        "    # Load visited links\n",
        "    with open(VISITED_LINKS_FILE, \"r\") as f:\n",
        "        visited_links = set(row[0] for row in csv.reader(f) if row)\n",
        "\n",
        "    print(f\"Found {len(category_links)} category links. Starting to scrape each category...\\n\")\n",
        "\n",
        "    for category_idx, category_url in enumerate(category_links, start=1):\n",
        "        print(f\"Processing category {category_idx}/{len(category_links)}: {category_url}\")\n",
        "\n",
        "        # Fetch card links within the category\n",
        "        card_links = get_card_links(category_url)\n",
        "        if not card_links:\n",
        "            print(f\"No card links found in category: {category_url}\")\n",
        "            continue\n",
        "\n",
        "        scraped_content = []\n",
        "        for idx, card_url in enumerate(card_links, start=1):\n",
        "            if card_url in visited_links:\n",
        "                print(f\"Skipping already visited link: {card_url}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Scraping card {idx}/{len(card_links)}: {card_url}\")\n",
        "            content = scrape_card_content(card_url)\n",
        "            if content:\n",
        "                scraped_content.append(content)\n",
        "                # Mark this link as visited\n",
        "                append_to_csv(VISITED_LINKS_FILE, [[card_url]])\n",
        "\n",
        "            # Combine and use model every two iterations\n",
        "            if len(scraped_content) == 2:\n",
        "                combined_text = \"\\n\".join(scraped_content)\n",
        "                print(\"Generating meaningful data using AI model...\")\n",
        "                time.sleep(15)  # Wait for 15 seconds before invoking the model\n",
        "                raw_response = create_medical_advices_from_text(combined_text)\n",
        "\n",
        "                try:\n",
        "                    # Clean the response and parse JSON\n",
        "                    clean_response = raw_response.strip().lstrip(\"```json\").rstrip(\"```\").strip()\n",
        "                    medical_data = json.loads(clean_response)\n",
        "\n",
        "                    # Process and save data\n",
        "                    advice_rows = []\n",
        "                    for entry in medical_data:\n",
        "                        advice_rows.append([\n",
        "                            entry[\"q_type\"],\n",
        "                            entry[\"question\"],\n",
        "                            entry[\"answer\"]\n",
        "                        ])\n",
        "\n",
        "                    append_to_csv(OUTPUT_CSV_FILE, advice_rows)\n",
        "                    print(\"Data generated and saved successfully.\")\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Failed to parse AI response: {e}\\nResponse: {raw_response}\")\n",
        "\n",
        "                # Reset scraped content\n",
        "                scraped_content = []\n",
        "\n",
        "    print(\"Scraping and data generation completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bHhTXAyNcvsy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GH7QQNtX_Vjq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}